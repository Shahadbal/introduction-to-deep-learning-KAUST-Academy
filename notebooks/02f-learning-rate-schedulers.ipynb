{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDT9h3egZqzr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection, preprocessing, pipeline\n",
        "import torch\n",
        "from torch import nn, optim, utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients_(\n",
        "    clip_grad_strategy,\n",
        "    model_fn,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "    if clip_grad_strategy == \"value\" and clip_value is not None:\n",
        "        nn.utils.clip_grad_value_(\n",
        "            model_fn.parameters(),\n",
        "            clip_value\n",
        "        )\n",
        "    elif clip_grad_strategy == \"norm\" and max_norm is not None:\n",
        "        nn.utils.clip_grad_norm_(\n",
        "            model_fn.parameters(),\n",
        "            max_norm,\n",
        "            norm_type,\n",
        "            error_if_nonfinite\n",
        "        )\n",
        "    elif clip_grad_strategy is None:\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "def compute_average_loss(dataloader, loss_fn, model_fn):\n",
        "    total_loss = torch.zeros(1, 1)\n",
        "    for features, targets in dataloader:\n",
        "        predictions = model_fn(features)\n",
        "        total_loss += loss_fn(predictions, targets)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def initialize_linear_layer(\n",
        "    in_features,\n",
        "    out_features,\n",
        "    init_strategy_=nn.init.kaiming_uniform_):\n",
        "    linear_layer = nn.Linear(in_features, out_features)\n",
        "    init_strategy_(linear_layer.weight)\n",
        "    return linear_layer\n",
        "\n",
        "\n",
        "def make_mlp_classifier(\n",
        "    input_size,\n",
        "    hidden_sizes=None,\n",
        "    output_size=2,\n",
        "    activation_fn=None,\n",
        "    init_strategy_=nn.init.kaiming_uniform_,\n",
        "    batch_normalization=False):\n",
        "    modules = []\n",
        "    hidden_sizes = [] if hidden_sizes is None else hidden_sizes\n",
        "    for hidden_size in hidden_sizes:\n",
        "        hidden_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            init_strategy_,\n",
        "        )\n",
        "        modules.append(hidden_layer)\n",
        "        if batch_normalization:\n",
        "            modules.append(nn.BatchNorm1d(hidden_size))\n",
        "        if activation_fn is not None:\n",
        "            modules.append(activation_fn)\n",
        "        input_size=hidden_size\n",
        "    output_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            output_size,\n",
        "            init_strategy_,\n",
        "    )\n",
        "    modules.append(output_layer)\n",
        "    modules.append(nn.LogSoftmax(dim=1))\n",
        "    model_fn = nn.Sequential(*modules)\n",
        "    return model_fn, nn.NLLLoss()\n"
      ],
      "metadata": {
        "id": "AnR4nFfYZLBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the training loop"
      ],
      "metadata": {
        "id": "WVVpUy_62cln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    clip_grad_strategy=None,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    lr_scheduler=None,\n",
        "    log_epochs=1,\n",
        "    max_epochs=1,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "\n",
        "    history = {\n",
        "        \"epoch\": [],\n",
        "        \"average_train_loss\": [],\n",
        "        \"average_val_loss\": [],\n",
        "        \"lr\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_train_loss = torch.zeros(1, 1)\n",
        "        model_fn = model_fn.train()\n",
        "        for features, targets in train_dataloader:\n",
        "\n",
        "            # forward pass\n",
        "            predictions = model_fn(features)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_train_loss += loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            clip_gradients_(\n",
        "                clip_grad_strategy,\n",
        "                model_fn,\n",
        "                clip_value,\n",
        "                error_if_nonfinite,\n",
        "                max_norm,\n",
        "                norm_type\n",
        "            )\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        average_train_loss = total_train_loss / len(train_dataloader)\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"average_train_loss\"].append(average_train_loss.item())\n",
        "\n",
        "        # validation after every training epoch\n",
        "        model_fn = model_fn.eval()\n",
        "        with torch.inference_mode():\n",
        "            average_val_loss = compute_average_loss(\n",
        "                val_dataloader,\n",
        "                loss_fn,\n",
        "                model_fn\n",
        "            )\n",
        "        history[\"average_val_loss\"].append(average_val_loss.item())\n",
        "\n",
        "        # update the learning rate after every training epoch\n",
        "        if lr_scheduler is not None:\n",
        "            history[\"lr\"].append(lr_scheduler.get_last_lr()[-1])\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        if epoch % log_epochs == 0:\n",
        "            message = f\"Epoch {epoch}, Average train Loss {average_train_loss.item():.4f}, Average val Loss {average_val_loss.item():.4f}\"\n",
        "            print(message)\n",
        "\n",
        "    history_df = (pd.DataFrame.from_dict(history)\n",
        "                              .set_index(\"epoch\"))\n",
        "    return history_df\n"
      ],
      "metadata": {
        "id": "m9fx_QUw2PpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the data"
      ],
      "metadata": {
        "id": "zRQSUa6c2q9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = 784\n",
        "OUTPUT_SIZE = 10\n",
        "\n",
        "_train_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_train_small.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n",
        "train_data, val_data = model_selection.train_test_split(\n",
        "    _train_data,\n",
        "    test_size=0.1,\n",
        "    stratify=_train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "test_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_test.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n"
      ],
      "metadata": {
        "id": "QRxVn9Kd2seV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.MinMaxScaler(),\n",
        "    preprocessing.FunctionTransformer(lambda arr: arr.astype(np.float32)),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n",
        "\n",
        "target_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.FunctionTransformer(lambda df: df.to_numpy()),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n"
      ],
      "metadata": {
        "id": "9i6znCRN2uYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# create the training dataset and dataloader\n",
        "train_features_tensor = features_preprocessor.fit_transform(\n",
        "    train_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "train_target_tensor = target_preprocessor.fit_transform(\n",
        "    train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "train_dataset = utils.data.TensorDataset(\n",
        "    train_features_tensor,\n",
        "    train_target_tensor\n",
        ")\n",
        "\n",
        "train_dataloader = utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "val_features_tensor = features_preprocessor.transform(\n",
        "    val_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "val_target_tensor = target_preprocessor.transform(\n",
        "    val_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "val_dataset = utils.data.TensorDataset(\n",
        "    val_features_tensor,\n",
        "    val_target_tensor\n",
        ")\n",
        "\n",
        "val_dataloader = utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the test dataset and dataloader\n",
        "test_features_tensor = features_preprocessor.transform(\n",
        "    test_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "test_target_tensor = target_preprocessor.transform(\n",
        "    test_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "test_dataset = utils.data.TensorDataset(\n",
        "    test_features_tensor,\n",
        "    test_target_tensor\n",
        ")\n",
        "\n",
        "test_dataloader = utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n"
      ],
      "metadata": {
        "id": "gUfPv-bB2uiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Rate Schedulers"
      ],
      "metadata": {
        "id": "Y3K2nqssZIe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.lr_scheduler."
      ],
      "metadata": {
        "id": "tZpel3jTbK75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Power Scheduling"
      ],
      "metadata": {
        "id": "ORIr9HdEZPeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.lr_scheduler.PolynomialLR?"
      ],
      "metadata": {
        "id": "N9LZEB5HZVPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE = 100\n",
        "LEARNING_RATE = 1e-1\n",
        "MAX_EPOCHS = 20\n",
        "\n",
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE],\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "lr_scheduler = optim.lr_scheduler.PolynomialLR(\n",
        "    optimizer,\n",
        "    total_iters=5,\n",
        "    power=1.0\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "aqWhQhfv2z6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "_ = (history.loc[:, [\"average_train_loss\", \"average_val_loss\"]]\n",
        "            .plot(ax=axes[0]))\n",
        "_ = (history.loc[:, [\"lr\"]]\n",
        "            .plot(ax=axes[1]))\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "YEym68Ouw8uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exponential Scheduling"
      ],
      "metadata": {
        "id": "GUGcY6_RZXcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.lr_scheduler.ExponentialLR?"
      ],
      "metadata": {
        "id": "T765_TLLZaJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE],\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "lr_scheduler = optim.lr_scheduler.ExponentialLR(\n",
        "    optimizer,\n",
        "    gamma=0.9\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "fqd6wl9ccDBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "_ = (history.loc[:, [\"average_train_loss\", \"average_val_loss\"]]\n",
        "            .plot(ax=axes[0]))\n",
        "_ = (history.loc[:, [\"lr\"]]\n",
        "            .plot(ax=axes[1]))\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "OQ9b4hqAsV0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-wise Constant Scheduling"
      ],
      "metadata": {
        "id": "ZqoGalIAZa7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.lr_scheduler.StepLR?"
      ],
      "metadata": {
        "id": "DcI6AIz_ZgGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE],\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=5,\n",
        "    gamma=0.1,\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "UTlPajp73-vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "_ = (history.loc[:, [\"average_train_loss\", \"average_val_loss\"]]\n",
        "            .plot(ax=axes[0]))\n",
        "_ = (history.loc[:, [\"lr\"]]\n",
        "            .plot(ax=axes[1]))\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "gi19cnEEsxKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-cycle Scheduling"
      ],
      "metadata": {
        "id": "8clzNnoxZlz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.lr_scheduler.OneCycleLR?"
      ],
      "metadata": {
        "id": "b_6KZgRlZnu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE],\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model_fn.parameters(),\n",
        "    lr=LEARNING_RATE / 10\n",
        ")\n",
        "\n",
        "lr_scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    epochs=MAX_EPOCHS,\n",
        "    steps_per_epoch=1, # because we update after each epoch!\n",
        "    max_lr=LEARNING_RATE,\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "hYPSX49j4Ki8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "_ = (history.loc[:, [\"average_train_loss\", \"average_val_loss\"]]\n",
        "            .plot(ax=axes[0]))\n",
        "_ = (history.loc[:, [\"lr\"]]\n",
        "            .plot(ax=axes[1]))\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "tDMFFf5Ds1dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Scheduling"
      ],
      "metadata": {
        "id": "i5LG_xWoZh-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optim.lr_scheduler.ReduceLROnPlateau?"
      ],
      "metadata": {
        "id": "AzNrzd3FZjh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    clip_grad_strategy=None,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    lr_scheduler=None,\n",
        "    log_epochs=1,\n",
        "    max_epochs=1,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "\n",
        "    history = {\n",
        "        \"epoch\": [],\n",
        "        \"average_train_loss\": [],\n",
        "        \"average_val_loss\": [],\n",
        "        \"lr\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_train_loss = torch.zeros(1, 1)\n",
        "        model_fn = model_fn.train()\n",
        "        for features, targets in train_dataloader:\n",
        "\n",
        "            # forward pass\n",
        "            predictions = model_fn(features)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_train_loss += loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            clip_gradients_(\n",
        "                clip_grad_strategy,\n",
        "                model_fn,\n",
        "                clip_value,\n",
        "                error_if_nonfinite,\n",
        "                max_norm,\n",
        "                norm_type\n",
        "            )\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        average_train_loss = total_train_loss / len(train_dataloader)\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"average_train_loss\"].append(average_train_loss.item())\n",
        "\n",
        "        # validation after every training epoch\n",
        "        model_fn = model_fn.eval()\n",
        "        with torch.inference_mode():\n",
        "            average_val_loss = compute_average_loss(\n",
        "                val_dataloader,\n",
        "                loss_fn,\n",
        "                model_fn\n",
        "            )\n",
        "        history[\"average_val_loss\"].append(average_val_loss.item())\n",
        "\n",
        "        # update the learning rate after every training epoch\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step(average_val_loss.item())\n",
        "            history[\"lr\"].append(lr_scheduler._last_lr[-1])\n",
        "\n",
        "        if epoch % log_epochs == 0:\n",
        "            message = f\"Epoch {epoch}, Average train Loss {average_train_loss.item():.4f}, Average val Loss {average_val_loss.item():.4f}\"\n",
        "            print(message)\n",
        "\n",
        "    history_df = (pd.DataFrame.from_dict(history)\n",
        "                              .set_index(\"epoch\"))\n",
        "    return history_df\n"
      ],
      "metadata": {
        "id": "cKQB8zOIwhbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE],\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode=\"min\",\n",
        "    patience=2,\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    lr_scheduler=lr_scheduler,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")\n"
      ],
      "metadata": {
        "id": "rSsnROvB4FIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "_ = (history.loc[:, [\"average_train_loss\", \"average_val_loss\"]]\n",
        "            .plot(ax=axes[0]))\n",
        "_ = (history.loc[:, [\"lr\"]]\n",
        "            .plot(ax=axes[1]))\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "p6ZpyStuv66w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise:\n",
        "\n",
        "Create a MLP with three hidden layers, each with 100 neurons per layer, and train it for 10 epochs using the Adam optimizer with the following learning rate schedulers.\n",
        "\n",
        "1. ExponentialLR\n",
        "2. ReduceLROnPlateau\n",
        "3. OneCycleLR\n",
        "\n",
        "Plot the learning rate schedule, as well as the training and validation loss curves. Compare and contrast."
      ],
      "metadata": {
        "id": "z5HdGJxCfvub"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A-9AnuuwgUKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}