{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install lightning torchinfo"
      ],
      "metadata": {
        "id": "uIjrxz1esmCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_kgBiGLSOEM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection, pipeline, preprocessing\n",
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "import torchinfo\n",
        "import lightning as L"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define some utility functions\n",
        "\n",
        "The code in the cell below defines a few utility functions that will make our life easier."
      ],
      "metadata": {
        "id": "rGHvJ6JPPNa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients_(\n",
        "    clip_grad_strategy,\n",
        "    model_fn,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "    if clip_grad_strategy == \"value\" and clip_value is not None:\n",
        "        nn.utils.clip_grad_value_(\n",
        "            model_fn.parameters(),\n",
        "            clip_value\n",
        "        )\n",
        "    elif clip_grad_strategy == \"norm\" and max_norm is not None:\n",
        "        nn.utils.clip_grad_norm_(\n",
        "            model_fn.parameters(),\n",
        "            max_norm,\n",
        "            norm_type,\n",
        "            error_if_nonfinite\n",
        "        )\n",
        "    elif clip_grad_strategy is None:\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "def compute_average_loss(dataloader, loss_fn, model_fn):\n",
        "    total_loss = torch.zeros(1, 1)\n",
        "    for features, targets in dataloader:\n",
        "        predictions = model_fn(features)\n",
        "        total_loss += loss_fn(predictions, targets)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def initialize_linear_layer(\n",
        "    in_features,\n",
        "    out_features,\n",
        "    init_strategy_=nn.init.kaiming_uniform_):\n",
        "    linear_layer = nn.Linear(in_features, out_features)\n",
        "    init_strategy_(linear_layer.weight)\n",
        "    return linear_layer\n",
        "\n",
        "\n",
        "def make_mlp_classifier(\n",
        "    input_size,\n",
        "    hidden_sizes=None,\n",
        "    output_size=2,\n",
        "    activation_fn=None,\n",
        "    init_strategy_=nn.init.kaiming_uniform_,\n",
        "    batch_normalization=False):\n",
        "    modules = []\n",
        "    hidden_sizes = [] if hidden_sizes is None else hidden_sizes\n",
        "    for hidden_size in hidden_sizes:\n",
        "        hidden_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            init_strategy_,\n",
        "        )\n",
        "        modules.append(hidden_layer)\n",
        "        if batch_normalization:\n",
        "            modules.append(nn.BatchNorm1d(hidden_size))\n",
        "        if activation_fn is not None:\n",
        "            modules.append(activation_fn)\n",
        "        input_size=hidden_size\n",
        "    output_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            output_size,\n",
        "            init_strategy_,\n",
        "    )\n",
        "    modules.append(output_layer)\n",
        "    modules.append(nn.LogSoftmax(dim=1))\n",
        "    model_fn = nn.Sequential(*modules)\n",
        "    return model_fn, nn.NLLLoss()\n"
      ],
      "metadata": {
        "id": "iBhCJguAJ9Cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    clip_grad_strategy=None,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    log_epochs=1,\n",
        "    max_epochs=1,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "\n",
        "    history = {\n",
        "        \"epoch\": [],\n",
        "        \"average_train_loss\": [],\n",
        "        \"average_val_loss\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_train_loss = torch.zeros(1, 1)\n",
        "        model_fn = model_fn.train()\n",
        "        for features, targets in train_dataloader:\n",
        "\n",
        "            # forward pass\n",
        "            predictions = model_fn(features)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_train_loss += loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            clip_gradients_(\n",
        "                clip_grad_strategy,\n",
        "                model_fn,\n",
        "                clip_value,\n",
        "                error_if_nonfinite,\n",
        "                max_norm,\n",
        "                norm_type\n",
        "            )\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        average_train_loss = total_train_loss / len(train_dataloader)\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"average_train_loss\"].append(average_train_loss.item())\n",
        "\n",
        "        # validation after every training epoch\n",
        "        model_fn = model_fn.eval()\n",
        "        with torch.inference_mode():\n",
        "            average_val_loss = compute_average_loss(\n",
        "                val_dataloader,\n",
        "                loss_fn,\n",
        "                model_fn\n",
        "            )\n",
        "        history[\"average_val_loss\"].append(average_val_loss.item())\n",
        "\n",
        "\n",
        "        if epoch % log_epochs == 0:\n",
        "            message = f\"Epoch {epoch}, Average train Loss {average_train_loss.item():.4f}, Average val Loss {average_val_loss.item():.4f}\"\n",
        "            print(message)\n",
        "\n",
        "    history_df = (pd.DataFrame.from_dict(history)\n",
        "                              .set_index(\"epoch\"))\n",
        "    return history_df\n"
      ],
      "metadata": {
        "id": "pzCDpcYgqavj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "In this section we will train a DNN model on the MNIST dataset and then use this pre-trained model as a starting point for training a model to classify an Arabic version of the MNIST dataset."
      ],
      "metadata": {
        "id": "uSEfC4aeSeTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the MNIST data"
      ],
      "metadata": {
        "id": "WNqB4QAmOLaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = 784\n",
        "OUTPUT_SIZE = 10\n",
        "\n",
        "_train_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_train_small.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n",
        "mnist_train_data, mnist_val_data = model_selection.train_test_split(\n",
        "    _train_data,\n",
        "    test_size=0.1,\n",
        "    stratify=_train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "mnist_test_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_test.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")"
      ],
      "metadata": {
        "id": "pR69GKYgI64h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create preprocessing pipelines"
      ],
      "metadata": {
        "id": "lEerhXJQOUoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.MinMaxScaler(),\n",
        "    preprocessing.FunctionTransformer(lambda arr: arr.astype(np.float32)),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n",
        "\n",
        "target_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.FunctionTransformer(lambda df: df.to_numpy()),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n"
      ],
      "metadata": {
        "id": "IOvwdgWvOJsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "iYDuB5zYOZL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# create the training dataset and dataloader\n",
        "_train_features_tensor = features_preprocessor.fit_transform(\n",
        "    mnist_train_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "_train_target_tensor = target_preprocessor.fit_transform(\n",
        "    mnist_train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_train_dataset = utils.data.TensorDataset(\n",
        "    _train_features_tensor,\n",
        "    _train_target_tensor\n",
        ")\n",
        "\n",
        "mnist_train_dataloader = utils.data.DataLoader(\n",
        "    _train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "_val_features_tensor = features_preprocessor.transform(\n",
        "    mnist_val_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "_val_target_tensor = target_preprocessor.transform(\n",
        "    mnist_val_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_val_dataset = utils.data.TensorDataset(\n",
        "    _val_features_tensor,\n",
        "    _val_target_tensor\n",
        ")\n",
        "\n",
        "mnist_val_dataloader = utils.data.DataLoader(\n",
        "    _val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the test dataset and dataloader\n",
        "_test_features_tensor = features_preprocessor.transform(\n",
        "    mnist_test_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "_test_target_tensor = target_preprocessor.transform(\n",
        "    mnist_test_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_test_dataset = utils.data.TensorDataset(\n",
        "    _test_features_tensor,\n",
        "    _test_target_tensor\n",
        ")\n",
        "\n",
        "mnist_test_dataloader = utils.data.DataLoader(\n",
        "    _test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n"
      ],
      "metadata": {
        "id": "3gIHandmN6MV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a DNN on the MNIST data"
      ],
      "metadata": {
        "id": "pPZ_cRdOOltl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE = 100\n",
        "LEARNING_RATE = 1e-2\n",
        "MAX_EPOCHS = 20\n",
        "\n",
        "pretrained_mnist_model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE,\n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE],\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    pretrained_mnist_model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    pretrained_mnist_model_fn,\n",
        "    optimizer,\n",
        "    mnist_train_dataloader,\n",
        "    mnist_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "cduU9-omuNxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Arabic Handwritten Digits Data\n",
        "\n",
        "Here we will download only the test dataset on 10k images. All images have the same size 28x28 = 784 pixels as the orginal MNIST data; there are also the same number of classes in this dataset."
      ],
      "metadata": {
        "id": "dZmowE54vJXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "gdown 1_aWjSUmpBlLSTSJF4xhmdZnzMoK4AMRO\n",
        "gdown 1Syln7zCy9Ue8x_F5qgq_0EHvaL_h_oL5\n",
        "gdown 1QQUsu7QciAZ6KXt6u-ZaMT_61iNjgy0a"
      ],
      "metadata": {
        "id": "PKuee-dFuwLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_test_features = pd.read_csv(\n",
        "    \"/content/csvTestImages 10k x 784.csv\",\n",
        "    header=None,\n",
        "    names=[f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n",
        "\n",
        "_test_target = pd.read_csv(\n",
        "    \"/content/csvTestLabel 10k x 1.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"],\n",
        ")\n",
        "_splits = model_selection.train_test_split(\n",
        "    _test_features,\n",
        "    _test_target,\n",
        "    test_size=0.1,\n",
        "    shuffle=True,\n",
        "    stratify=_test_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "arabic_mnist_train_features, arabic_mnist_val_features = _splits[:2]\n",
        "arabic_mnist_train_target, arabic_mnist_val_target = _splits[2:]"
      ],
      "metadata": {
        "id": "XY8q8UlWvPme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the training dataset and dataloader\n",
        "_train_features_tensor = features_preprocessor.fit_transform(\n",
        "    arabic_mnist_train_features\n",
        ")\n",
        "\n",
        "_train_target_tensor = target_preprocessor.fit_transform(\n",
        "    arabic_mnist_train_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_train_dataset = utils.data.TensorDataset(\n",
        "    _train_features_tensor,\n",
        "    _train_target_tensor\n",
        ")\n",
        "\n",
        "arabic_mnist_train_dataloader = utils.data.DataLoader(\n",
        "    _train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "_val_features_tensor = features_preprocessor.transform(\n",
        "    arabic_mnist_val_features)\n",
        "\n",
        "_val_target_tensor = target_preprocessor.transform(\n",
        "    arabic_mnist_val_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_val_dataset = utils.data.TensorDataset(\n",
        "    _val_features_tensor,\n",
        "    _val_target_tensor\n",
        ")\n",
        "\n",
        "arabic_mnist_val_dataloader = utils.data.DataLoader(\n",
        "    _val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")"
      ],
      "metadata": {
        "id": "s-lShez-wxo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Benchmark\n",
        "\n",
        "First, we can use logistic regression as a benchmark model. Our more elaborate models should seek to outperform this benchmark."
      ],
      "metadata": {
        "id": "u_KLLD7cPBK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "benchmark_model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE,\n",
        "    output_size=OUTPUT_SIZE,\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    benchmark_model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    benchmark_model_fn,\n",
        "    optimizer,\n",
        "    arabic_mnist_train_dataloader,\n",
        "    arabic_mnist_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "qywKNYomNMTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Freeze the pre-trained backbone and fine-tune the classifier head"
      ],
      "metadata": {
        "id": "1nSW7GMWOcJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(pretrained_mnist_model_fn)"
      ],
      "metadata": {
        "id": "i42qgFgcNMVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "\n",
        "arabic_mnist_model_fn = copy.deepcopy(pretrained_mnist_model_fn)\n",
        "\n",
        "# freeze layers 0 through 5 (inclusive!)\n",
        "for i in range(0, 5 + 1):\n",
        "    for p in arabic_mnist_model_fn[i].parameters():\n",
        "        p.requires_grad = False"
      ],
      "metadata": {
        "id": "For5MivXQEzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(arabic_mnist_model_fn)"
      ],
      "metadata": {
        "id": "rFQq3gU6iSSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(\n",
        "    arabic_mnist_model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    arabic_mnist_model_fn,\n",
        "    optimizer,\n",
        "    arabic_mnist_train_dataloader,\n",
        "    arabic_mnist_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "Q2UP6IX-OS0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just fine-tuning the classifier head didn't seem to work very well. Lets try to un-freeze additional layers."
      ],
      "metadata": {
        "id": "2xio2npylNSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_mnist_model_fn = copy.deepcopy(pretrained_mnist_model_fn)\n",
        "\n",
        "# freeze layers 0 through 3 (inclusive!)\n",
        "for i in range(0, 3 + 1):\n",
        "    for p in arabic_mnist_model_fn[i].parameters():\n",
        "        p.requires_grad = False"
      ],
      "metadata": {
        "id": "BxMjYozflVZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torchinfo.summary(arabic_mnist_model_fn)"
      ],
      "metadata": {
        "id": "APPiuoAwlVm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(\n",
        "    arabic_mnist_model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    arabic_mnist_model_fn,\n",
        "    optimizer,\n",
        "    arabic_mnist_train_dataloader,\n",
        "    arabic_mnist_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "TAUo1EgyljTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are seeing some improvement!"
      ],
      "metadata": {
        "id": "73mGhx22lyLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune the MNIST DNN\n",
        "\n",
        "You can also try unfreezing all of the layers and fine-tune the pre-trained model on your new data."
      ],
      "metadata": {
        "id": "OgqDE08Nxxgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_mnist_model_fn = copy.deepcopy(pretrained_mnist_model_fn)"
      ],
      "metadata": {
        "id": "j8FrlEyk3VvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(\n",
        "    arabic_mnist_model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    arabic_mnist_model_fn,\n",
        "    optimizer,\n",
        "    arabic_mnist_train_dataloader,\n",
        "    arabic_mnist_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "QycJW9wNuNz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Compare the performance of the original MNIST model at predicting the Arabic Handwritten digits with the performance of the Arabic MNIST model that was fine-tuned on the Arabic MNIST data."
      ],
      "metadata": {
        "id": "uaCQM6TCFemd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PA0fqxdSEylU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised pre-training\n",
        "\n",
        "Often you will have a large amount of unlabeled data and a small amount of labeled data. In this situation, one possible solution is to use unsupervised pre-training."
      ],
      "metadata": {
        "id": "7ELQU7KPsByE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_mnist_unlabeled_features = pd.read_csv(\n",
        "    \"/content/csvTrainImages 60k x 784.csv\",\n",
        "    header=None,\n",
        "    names=[f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n"
      ],
      "metadata": {
        "id": "xvB6WJuvTyiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate an embedding\n",
        "\n",
        "When applying unsupervised pre-training you first need to find a \"good\" embedding of your unlabeled features. For computer vision applications you would want to use more powerful models such as autoencoders, generative adversarial models (GANs) or similar to generate your embeddings. Here we just use PCA."
      ],
      "metadata": {
        "id": "WCgz8AkcGuD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import decomposition"
      ],
      "metadata": {
        "id": "B1fuFiik-SmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_pipeline = pipeline.make_pipeline(\n",
        "    decomposition.PCA(n_components=0.95),\n",
        ")"
      ],
      "metadata": {
        "id": "qaHp1XiAKwoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_embedding = encoder_pipeline.fit_transform(arabic_mnist_unlabeled_features)"
      ],
      "metadata": {
        "id": "cu_myI9gM_cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, input_size = feature_embedding.shape"
      ],
      "metadata": {
        "id": "Z5deJGowhcox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we define a \"good\" embedding? Reconstruction error! Once we have a feature embedding, we can invert the embedding transformation in order to reconstruct the original features. Once we have a reconstruction of the original features we can estimate the reconstruction error by comparing the original features and the reconstructed features.\n",
        "\n",
        "A good embedding will have a small reconstruction error."
      ],
      "metadata": {
        "id": "AjTpWoNzeX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "mWN595tQnukV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we estimate the reconstruction error using mean squared error\n",
        "reconstructed_features = encoder_pipeline.inverse_transform(feature_embedding)\n",
        "metrics.mean_squared_error(\n",
        "    arabic_mnist_unlabeled_features,\n",
        "    reconstructed_features\n",
        ")"
      ],
      "metadata": {
        "id": "pdBaq8x5dVs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "How could we improve the performance of our feature encoder pipeline? What impact will improving the performance of the feature encoder pipeline have on the performance of the DNN trained on the encoded features?"
      ],
      "metadata": {
        "id": "msdjiWEtIcgK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9AEeL0PizSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use encoder pipeline to embed your labeled data\n",
        "\n",
        "Hopefully our \"good\" embedding of the unlabeled features has learned useful information for our supervised classification task. Now we use the trained encoder pipeline to embed our labeled features."
      ],
      "metadata": {
        "id": "lGKvB5DqJEFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_train_features = encoder_pipeline.transform(arabic_mnist_train_features)\n",
        "embedded_val_features = encoder_pipeline.transform(arabic_mnist_val_features)\n"
      ],
      "metadata": {
        "id": "Yl6cxhRVn_R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to prepare out data for our DNN."
      ],
      "metadata": {
        "id": "Q9jbyFuaoC65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.StandardScaler(),\n",
        "    preprocessing.FunctionTransformer(lambda arr: arr.astype(np.float32)),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n",
        "\n",
        "target_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.FunctionTransformer(lambda df: df.to_numpy()),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n",
        "\n",
        "# create the training dataset and dataloader\n",
        "_train_features_tensor = features_preprocessor.fit_transform(\n",
        "    embedded_train_features\n",
        ")\n",
        "\n",
        "_train_target_tensor = target_preprocessor.fit_transform(\n",
        "    arabic_mnist_train_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_train_dataset = utils.data.TensorDataset(\n",
        "    _train_features_tensor,\n",
        "    _train_target_tensor\n",
        ")\n",
        "\n",
        "embedded_train_dataloader = utils.data.DataLoader(\n",
        "    _train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "_val_features_tensor = features_preprocessor.transform(\n",
        "    embedded_val_features)\n",
        "\n",
        "_val_target_tensor = target_preprocessor.transform(\n",
        "    arabic_mnist_val_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_val_dataset = utils.data.TensorDataset(\n",
        "    _val_features_tensor,\n",
        "    _val_target_tensor\n",
        ")\n",
        "\n",
        "embedded_val_dataloader = utils.data.DataLoader(\n",
        "    _val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")"
      ],
      "metadata": {
        "id": "HOIBpvZvDI8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a DNN using the embedded features"
      ],
      "metadata": {
        "id": "8RXrE_ZeLsav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unsupervised_pretraining_model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=input_size,\n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE],\n",
        "    output_size=OUTPUT_SIZE,\n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(\n",
        "    unsupervised_pretraining_model_fn.parameters(),\n",
        "    lr=LEARNING_RATE\n",
        ")\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    unsupervised_pretraining_model_fn,\n",
        "    optimizer,\n",
        "    embedded_train_dataloader,\n",
        "    embedded_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "o2C_olL-KWKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Compare the performance of the transfer learning and unsupervised pre-training approaches to classifying the Arabic Handwritten Digits images."
      ],
      "metadata": {
        "id": "4vw1w-oNZReo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8KdL3FD0X2Y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}