{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIjrxz1esmCH",
        "outputId": "7663b0ed-ff4a-41b8-aca5-fde6c6ffb2b3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
            "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (6.3.0)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (2023.3.0)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.9/dist-packages (from lightning) (6.0)\n",
            "Requirement already satisfied: fastapi<0.89.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.88.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (4.5.0)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (4.11.2)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/dist-packages (from lightning) (2.0.1)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from lightning) (23.0)\n",
            "Requirement already satisfied: arrow<3.0,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.2.3)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (13.3.3)\n",
            "Requirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.5.1)\n",
            "Requirement already satisfied: websockets<12.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (11.0.1)\n",
            "Requirement already satisfied: torchmetrics<2.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.11.4)\n",
            "Requirement already satisfied: uvicorn<2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.21.1)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (5.7.1)\n",
            "Requirement already satisfied: pydantic<3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.10.7)\n",
            "Requirement already satisfied: starlette<2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.22.0)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (4.65.0)\n",
            "Requirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (3.1.2)\n",
            "Requirement already satisfied: lightning-cloud>=0.5.31 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.5.32)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (5.9.4)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (2.0.0+cu118)\n",
            "Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.26.15)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (8.1.3)\n",
            "Requirement already satisfied: inquirer<5.0,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (3.1.3)\n",
            "Requirement already satisfied: starsessions<2.0,>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.3.0)\n",
            "Requirement already satisfied: requests<4.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (2.27.1)\n",
            "Requirement already satisfied: croniter<1.4.0,>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.3.8)\n",
            "Requirement already satisfied: dateutils<2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.6.12)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.9/dist-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from dateutils<2.0->lightning) (2022.7.1)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /usr/local/lib/python3.9/dist-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<2.0->lightning) (3.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.8.4)\n",
            "Requirement already satisfied: blessed>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
            "Requirement already satisfied: python-editor>=1.0.4 in /usr/local/lib/python3.9/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
            "Requirement already satisfied: readchar>=3.0.6 in /usr/local/lib/python3.9/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2<5.0->lightning) (2.1.2)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.9/dist-packages (from lightning-cloud>=0.5.31->lightning) (2.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from lightning-cloud>=0.5.31->lightning) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<4.0->lightning) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<4.0->lightning) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<4.0->lightning) (2.0.12)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich<15.0,>=12.3.0->lightning) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich<15.0,>=12.3.0->lightning) (2.14.0)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (3.10.7)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning) (16.0.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (22.2.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.9/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.9/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (67.6.1)\n",
            "Requirement already satisfied: email-validator>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (0.0.6)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (5.7.0)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (3.8.9)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (0.23.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: dnspython>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from email-validator>=1.1.1->fastapi<0.89.0->lightning) (2.3.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.23.0->fastapi<0.89.0->lightning) (0.16.3)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.23.0->fastapi<0.89.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (1.0.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.5.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.17.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.19.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y_kgBiGLSOEM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection, pipeline, preprocessing\n",
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "import lightning as L"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define some utility functions\n",
        "\n",
        "The code in the cell below defines a few utility functions that will make our life easier."
      ],
      "metadata": {
        "id": "rGHvJ6JPPNa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients_(\n",
        "    clip_grad_strategy,\n",
        "    model_fn,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "    if clip_grad_strategy == \"value\" and clip_value is not None:\n",
        "        nn.utils.clip_grad_value_(\n",
        "            model_fn.parameters(),\n",
        "            clip_value\n",
        "        )\n",
        "    elif clip_grad_strategy == \"norm\" and max_norm is not None:\n",
        "        nn.utils.clip_grad_norm_(\n",
        "            model_fn.parameters(),\n",
        "            max_norm,\n",
        "            norm_type,\n",
        "            error_if_nonfinite\n",
        "        )\n",
        "    elif clip_grad_strategy is None:\n",
        "        pass\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "def compute_average_loss(dataloader, loss_fn, model_fn):\n",
        "    total_loss = torch.zeros(1, 1)\n",
        "    for features, targets in dataloader:\n",
        "        predictions = model_fn(features)        \n",
        "        total_loss += loss_fn(predictions, targets)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def initialize_linear_layer(\n",
        "    in_features,\n",
        "    out_features,\n",
        "    init_strategy_=nn.init.kaiming_uniform_):\n",
        "    linear_layer = nn.Linear(in_features, out_features)\n",
        "    init_strategy_(linear_layer.weight)\n",
        "    return linear_layer\n",
        "\n",
        "\n",
        "def make_mlp_classifier(\n",
        "    input_size,\n",
        "    hidden_sizes=None,\n",
        "    output_size=2,\n",
        "    activation_fn=None,\n",
        "    init_strategy_=nn.init.kaiming_uniform_,\n",
        "    batch_normalization=False):\n",
        "    modules = []\n",
        "    hidden_sizes = [] if hidden_sizes is None else hidden_sizes\n",
        "    for hidden_size in hidden_sizes:\n",
        "        hidden_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            init_strategy_,\n",
        "        )\n",
        "        modules.append(hidden_layer)\n",
        "        if batch_normalization:\n",
        "            modules.append(nn.BatchNorm1d(hidden_size))\n",
        "        if activation_fn is not None:\n",
        "            modules.append(activation_fn)\n",
        "        input_size=hidden_size\n",
        "    output_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            output_size,\n",
        "            init_strategy_,\n",
        "    )\n",
        "    modules.append(output_layer)\n",
        "    modules.append(nn.LogSoftmax(dim=1))\n",
        "    model_fn = nn.Sequential(*modules)\n",
        "    return model_fn, nn.NLLLoss()\n"
      ],
      "metadata": {
        "id": "iBhCJguAJ9Cq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    clip_grad_strategy=None,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    log_epochs=1,\n",
        "    max_epochs=1,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "  \n",
        "    history = {\n",
        "        \"epoch\": [],\n",
        "        \"average_train_loss\": [],\n",
        "        \"average_val_loss\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_train_loss = torch.zeros(1, 1)\n",
        "        model_fn = model_fn.train()\n",
        "        for features, targets in train_dataloader:\n",
        "            \n",
        "            # forward pass\n",
        "            predictions = model_fn(features)        \n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_train_loss += loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            clip_gradients_(\n",
        "                clip_grad_strategy,\n",
        "                model_fn,\n",
        "                clip_value,\n",
        "                error_if_nonfinite,\n",
        "                max_norm,\n",
        "                norm_type\n",
        "            )\n",
        "            optimizer.step()        \n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        average_train_loss = total_train_loss / len(train_dataloader)\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"average_train_loss\"].append(average_train_loss.item())\n",
        "\n",
        "        # validation after every training epoch\n",
        "        model_fn = model_fn.eval()\n",
        "        with torch.inference_mode():\n",
        "            average_val_loss = compute_average_loss(\n",
        "                val_dataloader,\n",
        "                loss_fn,\n",
        "                model_fn\n",
        "            )\n",
        "        history[\"average_val_loss\"].append(average_val_loss.item())\n",
        "\n",
        "\n",
        "        if epoch % log_epochs == 0:\n",
        "            message = f\"Epoch {epoch}, Average train Loss {average_train_loss.item():.4f}, Average val Loss {average_val_loss.item():.4f}\"\n",
        "            print(message)\n",
        "\n",
        "    history_df = (pd.DataFrame.from_dict(history)\n",
        "                              .set_index(\"epoch\"))\n",
        "    return history_df\n"
      ],
      "metadata": {
        "id": "pzCDpcYgqavj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "In this section we will train a DNN model on the MNIST dataset and then use this pre-trained model as a starting point for training a model to classify an Arabic version of the MNIST dataset."
      ],
      "metadata": {
        "id": "uSEfC4aeSeTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the MNIST data"
      ],
      "metadata": {
        "id": "WNqB4QAmOLaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = 784\n",
        "OUTPUT_SIZE = 10\n",
        "\n",
        "_train_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_train_small.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n",
        "mnist_train_data, mnist_val_data = model_selection.train_test_split(\n",
        "    _train_data,\n",
        "    test_size=0.1,\n",
        "    stratify=_train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "mnist_test_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_test.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")"
      ],
      "metadata": {
        "id": "pR69GKYgI64h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create preprocessing pipelines"
      ],
      "metadata": {
        "id": "lEerhXJQOUoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.MinMaxScaler(),\n",
        "    preprocessing.FunctionTransformer(lambda arr: arr.astype(np.float32)),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n",
        "\n",
        "target_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.FunctionTransformer(lambda df: df.to_numpy()),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n"
      ],
      "metadata": {
        "id": "IOvwdgWvOJsk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "iYDuB5zYOZL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# create the training dataset and dataloader\n",
        "_train_features_tensor = features_preprocessor.fit_transform(\n",
        "    mnist_train_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "_train_target_tensor = target_preprocessor.fit_transform(\n",
        "    mnist_train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_train_dataset = utils.data.TensorDataset(\n",
        "    _train_features_tensor,\n",
        "    _train_target_tensor\n",
        ")\n",
        "\n",
        "mnist_train_dataloader = utils.data.DataLoader(\n",
        "    _train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "_val_features_tensor = features_preprocessor.transform(\n",
        "    mnist_val_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "_val_target_tensor = target_preprocessor.transform(\n",
        "    mnist_val_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_val_dataset = utils.data.TensorDataset(\n",
        "    _val_features_tensor,\n",
        "    _val_target_tensor\n",
        ")\n",
        "\n",
        "mnist_val_dataloader = utils.data.DataLoader(\n",
        "    _val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the test dataset and dataloader\n",
        "_test_features_tensor = features_preprocessor.transform(\n",
        "    mnist_test_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "_test_target_tensor = target_preprocessor.transform(\n",
        "    mnist_test_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_test_dataset = utils.data.TensorDataset(\n",
        "    _test_features_tensor,\n",
        "    _test_target_tensor\n",
        ")\n",
        "\n",
        "mnist_test_dataloader = utils.data.DataLoader(\n",
        "    _test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n"
      ],
      "metadata": {
        "id": "3gIHandmN6MV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train a DNN on the MNIST data"
      ],
      "metadata": {
        "id": "pPZ_cRdOOltl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE = 100\n",
        "LEARNING_RATE = 1e-2\n",
        "MAX_EPOCHS = 20\n",
        "\n",
        "pretrained_mnist_model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(pretrained_mnist_model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    pretrained_mnist_model_fn,\n",
        "    optimizer,\n",
        "    mnist_train_dataloader,\n",
        "    mnist_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cduU9-omuNxi",
        "outputId": "e2e7d541-1b6e-41af-bbb3-baa9a2c448d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 0.7595, Average val Loss 0.4884\n",
            "Epoch 1, Average train Loss 0.3964, Average val Loss 0.4094\n",
            "Epoch 2, Average train Loss 0.3372, Average val Loss 0.3775\n",
            "Epoch 3, Average train Loss 0.3066, Average val Loss 0.3588\n",
            "Epoch 4, Average train Loss 0.2859, Average val Loss 0.3458\n",
            "Epoch 5, Average train Loss 0.2700, Average val Loss 0.3359\n",
            "Epoch 6, Average train Loss 0.2569, Average val Loss 0.3281\n",
            "Epoch 7, Average train Loss 0.2455, Average val Loss 0.3213\n",
            "Epoch 8, Average train Loss 0.2354, Average val Loss 0.3156\n",
            "Epoch 9, Average train Loss 0.2262, Average val Loss 0.3101\n",
            "Epoch 10, Average train Loss 0.2176, Average val Loss 0.3051\n",
            "Epoch 11, Average train Loss 0.2096, Average val Loss 0.3005\n",
            "Epoch 12, Average train Loss 0.2021, Average val Loss 0.2961\n",
            "Epoch 13, Average train Loss 0.1950, Average val Loss 0.2920\n",
            "Epoch 14, Average train Loss 0.1883, Average val Loss 0.2880\n",
            "Epoch 15, Average train Loss 0.1819, Average val Loss 0.2845\n",
            "Epoch 16, Average train Loss 0.1758, Average val Loss 0.2812\n",
            "Epoch 17, Average train Loss 0.1701, Average val Loss 0.2783\n",
            "Epoch 18, Average train Loss 0.1645, Average val Loss 0.2755\n",
            "Epoch 19, Average train Loss 0.1593, Average val Loss 0.2729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Arabic Handwritten Digits Data\n",
        "\n",
        "Here we will download only the test dataset on 10k images. All images have the same size 28x28 = 784 pixels as the orginal MNIST data; there are also the same number of classes in this dataset."
      ],
      "metadata": {
        "id": "dZmowE54vJXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO INSERT CODE HERE!"
      ],
      "metadata": {
        "id": "PKuee-dFuwLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_test_features = pd.read_csv(\n",
        "    \"/content/csvTestImages 10k x 784.csv\",\n",
        "    header=None,\n",
        "    names=[f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n",
        "\n",
        "_test_target = pd.read_csv(\n",
        "    \"/content/csvTestLabel 10k x 1.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"],\n",
        ")\n",
        "_splits = model_selection.train_test_split(\n",
        "    _test_features,\n",
        "    _test_target,\n",
        "    test_size=0.1,\n",
        "    shuffle=True,\n",
        "    stratify=_test_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "arabic_mnist_train_features, arabic_mnist_val_features = _splits[:2]\n",
        "arabic_mnist_train_target, arabic_mnist_val_target = _splits[2:]"
      ],
      "metadata": {
        "id": "XY8q8UlWvPme"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the training dataset and dataloader\n",
        "_train_features_tensor = features_preprocessor.fit_transform(\n",
        "    arabic_mnist_train_features\n",
        ")\n",
        "\n",
        "_train_target_tensor = target_preprocessor.fit_transform(\n",
        "    arabic_mnist_train_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_train_dataset = utils.data.TensorDataset(\n",
        "    _train_features_tensor,\n",
        "    _train_target_tensor\n",
        ")\n",
        "\n",
        "arabic_mnist_train_dataloader = utils.data.DataLoader(\n",
        "    _train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "_val_features_tensor = features_preprocessor.transform(\n",
        "    arabic_mnist_val_features)\n",
        "\n",
        "_val_target_tensor = target_preprocessor.transform(\n",
        "    arabic_mnist_val_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_val_dataset = utils.data.TensorDataset(\n",
        "    _val_features_tensor,\n",
        "    _val_target_tensor\n",
        ")\n",
        "\n",
        "arabic_mnist_val_dataloader = utils.data.DataLoader(\n",
        "    _val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")"
      ],
      "metadata": {
        "id": "s-lShez-wxo3"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune the MNIST DNN\n",
        "\n",
        "Since Arabic MNIST images have the exact same size as the original MNIST images, and the number of classes is also the same, we can just take the pre-trained MNIST classifer and fine-tune it using Arabic MNIST images. "
      ],
      "metadata": {
        "id": "OgqDE08Nxxgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_mnist_model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "# initialize model weights using MNIST DNN weights\n",
        "_pretrained_state_dict = pretrained_mnist_model_fn.state_dict()\n",
        "arabic_mnist_model_fn.load_state_dict(_pretrained_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8FrlEyk3VvK",
        "outputId": "d934ecce-89b6-429a-be52-1e538411d3f8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.SGD(arabic_mnist_model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    arabic_mnist_model_fn,\n",
        "    optimizer,\n",
        "    arabic_mnist_train_dataloader,\n",
        "    arabic_mnist_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QycJW9wNuNz4",
        "outputId": "5214c369-3f4e-4643-9650-a46e7a1d9bdb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 0.9494, Average val Loss 0.4363\n",
            "Epoch 1, Average train Loss 0.2840, Average val Loss 0.3166\n",
            "Epoch 2, Average train Loss 0.2092, Average val Loss 0.2694\n",
            "Epoch 3, Average train Loss 0.1729, Average val Loss 0.2433\n",
            "Epoch 4, Average train Loss 0.1505, Average val Loss 0.2265\n",
            "Epoch 5, Average train Loss 0.1349, Average val Loss 0.2146\n",
            "Epoch 6, Average train Loss 0.1231, Average val Loss 0.2058\n",
            "Epoch 7, Average train Loss 0.1139, Average val Loss 0.1989\n",
            "Epoch 8, Average train Loss 0.1064, Average val Loss 0.1934\n",
            "Epoch 9, Average train Loss 0.1000, Average val Loss 0.1889\n",
            "Epoch 10, Average train Loss 0.0945, Average val Loss 0.1852\n",
            "Epoch 11, Average train Loss 0.0897, Average val Loss 0.1821\n",
            "Epoch 12, Average train Loss 0.0854, Average val Loss 0.1795\n",
            "Epoch 13, Average train Loss 0.0816, Average val Loss 0.1773\n",
            "Epoch 14, Average train Loss 0.0781, Average val Loss 0.1754\n",
            "Epoch 15, Average train Loss 0.0749, Average val Loss 0.1737\n",
            "Epoch 16, Average train Loss 0.0720, Average val Loss 0.1722\n",
            "Epoch 17, Average train Loss 0.0693, Average val Loss 0.1710\n",
            "Epoch 18, Average train Loss 0.0667, Average val Loss 0.1699\n",
            "Epoch 19, Average train Loss 0.0644, Average val Loss 0.1689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Compare the performance of the original MNIST model at predicting the Arabic Handwritten digits with the performance of the Arabic MNIST model that was fine-tuned on the Arabic MNIST data."
      ],
      "metadata": {
        "id": "uaCQM6TCFemd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PA0fqxdSEylU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised pre-training\n",
        "\n",
        "Often you will have a large amount of unlabeled data and a small amount of labeled data. In this situation, one possible solution is to use unsupervised pre-training."
      ],
      "metadata": {
        "id": "7ELQU7KPsByE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arabic_mnist_unlabeled_features = pd.read_csv(\n",
        "    \"/content/csvTrainImages 60k x 784.csv\",\n",
        "    header=None,\n",
        "    names=[f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n"
      ],
      "metadata": {
        "id": "xvB6WJuvTyiL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate an embedding\n",
        "\n",
        "When applying unsupervised pre-training you first need to find a \"good\" embedding of your unlabeled features. For computer vision applications you would want to use more powerful models such as autoencoders, generative adversarial models (GANs) or similar to generate your embeddings. Here we just use PCA."
      ],
      "metadata": {
        "id": "WCgz8AkcGuD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import decomposition"
      ],
      "metadata": {
        "id": "B1fuFiik-SmE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_pipeline = pipeline.make_pipeline(\n",
        "    decomposition.PCA(n_components=0.95),\n",
        ")"
      ],
      "metadata": {
        "id": "qaHp1XiAKwoE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_embedding = encoder_pipeline.fit_transform(arabic_mnist_unlabeled_features)"
      ],
      "metadata": {
        "id": "cu_myI9gM_cG"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, input_size = feature_embedding.shape"
      ],
      "metadata": {
        "id": "Z5deJGowhcox"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we define a \"good\" embedding? Reconstruction error! Once we have a feature embedding, we can invert the embedding transformation in order to reconstruct the original features. Once we have a reconstruction of the original features we can estimate the reconstruction error by comparing the original features and the reconstructed features. \n",
        "\n",
        "A good embedding will have a small reconstruction error."
      ],
      "metadata": {
        "id": "AjTpWoNzeX_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "mWN595tQnukV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here we estimate the reconstruction error using mean squared error\n",
        "reconstructed_features = encoder_pipeline.inverse_transform(feature_embedding)\n",
        "metrics.mean_squared_error(\n",
        "    arabic_mnist_unlabeled_features,\n",
        "    reconstructed_features\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdBaq8x5dVs-",
        "outputId": "50f1e51f-9a79-4c46-bd5c-dc7cc25eff04"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192.25560211574674"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "How could we improve the performance of our feature encoder pipeline? What impact will improving the performance of the feature encoder pipeline have on the performance of the DNN trained on the encoded features?"
      ],
      "metadata": {
        "id": "msdjiWEtIcgK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9AEeL0PizSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use encoder pipeline to embed your labeled data\n",
        "\n",
        "Hopefully our \"good\" embedding of the unlabeled features has learned useful information for our supervised classification task. Now we use the trained encoder pipeline to embed our labeled features."
      ],
      "metadata": {
        "id": "lGKvB5DqJEFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the training dataset and dataloader\n",
        "_embedded_train_features = encoder_pipeline.transform(arabic_mnist_train_features)\n",
        "_train_features_tensor = features_preprocessor.fit_transform(\n",
        "    _embedded_train_features\n",
        ")\n",
        "\n",
        "_train_target_tensor = target_preprocessor.fit_transform(\n",
        "    arabic_mnist_train_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_train_dataset = utils.data.TensorDataset(\n",
        "    _train_features_tensor,\n",
        "    _train_target_tensor\n",
        ")\n",
        "\n",
        "embedded_train_dataloader = utils.data.DataLoader(\n",
        "    _train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "_embedded_val_features = encoder_pipeline.transform(arabic_mnist_val_features)\n",
        "_val_features_tensor = features_preprocessor.transform(\n",
        "    _embedded_val_features)\n",
        "\n",
        "_val_target_tensor = target_preprocessor.transform(\n",
        "    arabic_mnist_val_target.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "_val_dataset = utils.data.TensorDataset(\n",
        "    _val_features_tensor,\n",
        "    _val_target_tensor\n",
        ")\n",
        "\n",
        "embedded_val_dataloader = utils.data.DataLoader(\n",
        "    _val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")"
      ],
      "metadata": {
        "id": "HOIBpvZvDI8W"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a DNN using the embedded features"
      ],
      "metadata": {
        "id": "8RXrE_ZeLsav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unsupervised_pretraining_model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=input_size, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.SELU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(unsupervised_pretraining_model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    unsupervised_pretraining_model_fn,\n",
        "    optimizer,\n",
        "    embedded_train_dataloader,\n",
        "    embedded_val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "o2C_olL-KWKH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719b8017-5072-4b36-d6b4-5521fc22447d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.2518, Average val Loss 2.1412\n",
            "Epoch 1, Average train Loss 2.0201, Average val Loss 1.9120\n",
            "Epoch 2, Average train Loss 1.7352, Average val Loss 1.6060\n",
            "Epoch 3, Average train Loss 1.4126, Average val Loss 1.2500\n",
            "Epoch 4, Average train Loss 1.1201, Average val Loss 0.9848\n",
            "Epoch 5, Average train Loss 0.8801, Average val Loss 0.7840\n",
            "Epoch 6, Average train Loss 0.6943, Average val Loss 0.6362\n",
            "Epoch 7, Average train Loss 0.5602, Average val Loss 0.5313\n",
            "Epoch 8, Average train Loss 0.4649, Average val Loss 0.4524\n",
            "Epoch 9, Average train Loss 0.3998, Average val Loss 0.3968\n",
            "Epoch 10, Average train Loss 0.3530, Average val Loss 0.3547\n",
            "Epoch 11, Average train Loss 0.3166, Average val Loss 0.3246\n",
            "Epoch 12, Average train Loss 0.2880, Average val Loss 0.3025\n",
            "Epoch 13, Average train Loss 0.2632, Average val Loss 0.2850\n",
            "Epoch 14, Average train Loss 0.2429, Average val Loss 0.2689\n",
            "Epoch 15, Average train Loss 0.2249, Average val Loss 0.2563\n",
            "Epoch 16, Average train Loss 0.2114, Average val Loss 0.2448\n",
            "Epoch 17, Average train Loss 0.1984, Average val Loss 0.2352\n",
            "Epoch 18, Average train Loss 0.1871, Average val Loss 0.2265\n",
            "Epoch 19, Average train Loss 0.1780, Average val Loss 0.2191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Compare the performance of the transfer learning and unsupervised pre-training approaches to classifying the Arabic Handwritten Digits images."
      ],
      "metadata": {
        "id": "4vw1w-oNZReo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8KdL3FD0X2Y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}