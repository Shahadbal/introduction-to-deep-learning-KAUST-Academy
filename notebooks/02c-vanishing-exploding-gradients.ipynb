{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "\n",
        "pip install lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIjrxz1esmCH",
        "outputId": "7d2ec0fe-f30a-411e-f1e5-855a696c2dc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightning in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
            "Requirement already satisfied: inquirer<5.0,>=2.10.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (3.1.3)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (5.7.1)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (6.0)\n",
            "Requirement already satisfied: uvicorn<2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.21.1)\n",
            "Requirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.5.1)\n",
            "Requirement already satisfied: starlette<2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.22.0)\n",
            "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (4.65.0)\n",
            "Requirement already satisfied: packaging<25.0,>=17.1 in /usr/local/lib/python3.9/dist-packages (from lightning) (23.0)\n",
            "Requirement already satisfied: fsspec<2024.0,>=2022.5.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (2023.3.0)\n",
            "Requirement already satisfied: dateutils<2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.6.12)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (4.5.0)\n",
            "Requirement already satisfied: pydantic<3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.10.7)\n",
            "Requirement already satisfied: websockets<12.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (11.0.1)\n",
            "Requirement already satisfied: lightning-cloud>=0.5.31 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.5.32)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (5.9.4)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.8.0)\n",
            "Requirement already satisfied: croniter<1.4.0,>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.3.8)\n",
            "Requirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (3.1.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (8.1.3)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (13.3.3)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchmetrics<2.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.11.4)\n",
            "Requirement already satisfied: deepdiff<8.0,>=5.7.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (6.3.0)\n",
            "Requirement already satisfied: requests<4.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (2.27.1)\n",
            "Requirement already satisfied: fastapi<0.89.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (0.88.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.9/dist-packages (from lightning) (2.0.1)\n",
            "Requirement already satisfied: starsessions<2.0,>=1.2.1 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.3.0)\n",
            "Requirement already satisfied: arrow<3.0,>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.2.3)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (4.11.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.22.4)\n",
            "Requirement already satisfied: urllib3<3.0 in /usr/local/lib/python3.9/dist-packages (from lightning) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.9/dist-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.9/dist-packages (from dateutils<2.0->lightning) (2022.7.1)\n",
            "Requirement already satisfied: ordered-set<4.2.0,>=4.0.2 in /usr/local/lib/python3.9/dist-packages (from deepdiff<8.0,>=5.7.0->lightning) (4.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from starlette<2.0->lightning) (3.6.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec<2024.0,>=2022.5.0->lightning) (3.8.4)\n",
            "Requirement already satisfied: blessed>=1.19.0 in /usr/local/lib/python3.9/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (1.20.0)\n",
            "Requirement already satisfied: readchar>=3.0.6 in /usr/local/lib/python3.9/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (4.0.5)\n",
            "Requirement already satisfied: python-editor>=1.0.4 in /usr/local/lib/python3.9/dist-packages (from inquirer<5.0,>=2.10.0->lightning) (1.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from Jinja2<5.0->lightning) (2.1.2)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.9/dist-packages (from lightning-cloud>=0.5.31->lightning) (2.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from lightning-cloud>=0.5.31->lightning) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<4.0->lightning) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<4.0->lightning) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<4.0->lightning) (3.4)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from rich<15.0,>=12.3.0->lightning) (2.14.0)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from rich<15.0,>=12.3.0->lightning) (2.2.0)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.9/dist-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (3.10.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch<4.0,>=1.11.0->lightning) (1.11.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning) (3.25.2)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.14.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (1.8.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2024.0,>=2022.5.0->lightning) (22.2.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/dist-packages (from anyio<5,>=3.4.0->starlette<2.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.9/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.9/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (67.6.1)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (3.8.9)\n",
            "Requirement already satisfied: python-multipart>=0.0.5 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (0.0.6)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (5.7.0)\n",
            "Requirement already satisfied: email-validator>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from fastapi<0.89.0->lightning) (0.23.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.3.0)\n",
            "Requirement already satisfied: dnspython>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from email-validator>=1.1.1->fastapi<0.89.0->lightning) (2.3.0)\n",
            "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.23.0->fastapi<0.89.0->lightning) (1.5.0)\n",
            "Requirement already satisfied: httpcore<0.17.0,>=0.15.0 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.23.0->fastapi<0.89.0->lightning) (0.16.3)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.5.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.17.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (1.0.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.9/dist-packages (from uvicorn<2.0->lightning) (0.19.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y_kgBiGLSOEM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import model_selection, pipeline, preprocessing\n",
        "import torch\n",
        "from torch import nn, optim, utils\n",
        "import lightning as L"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vanishing and Exploding Gradients\n",
        "\n",
        "**Vanishing gradients**: gradients get smaller and smaller until parameters in early layers get updates so small that the model effectively stops learning. When this happens the training process fails to converge to a good solution. \n",
        "\n",
        "**Exploding gradients**: gradients get bigger and bigger until the paramters get updates so large that the training process begins to diverge!\n",
        "\n"
      ],
      "metadata": {
        "id": "uSEfC4aeSeTe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define some utility functions\n",
        "\n",
        "The code in the cell below defines a few utility functions that will make our life easier."
      ],
      "metadata": {
        "id": "rGHvJ6JPPNa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_average_loss(dataloader, loss_fn, model_fn):\n",
        "    total_loss = torch.zeros(1, 1)\n",
        "    for features, targets in dataloader:\n",
        "        predictions = model_fn(features)        \n",
        "        total_loss += loss_fn(predictions, targets)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    log_epochs=1,\n",
        "    max_epochs=1):\n",
        "  \n",
        "    history = {\n",
        "        \"epoch\": [],\n",
        "        \"average_train_loss\": [],\n",
        "        \"average_val_loss\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_train_loss = torch.zeros(1, 1)\n",
        "        model_fn = model_fn.train()\n",
        "        for features, targets in train_dataloader:\n",
        "            \n",
        "            # forward pass\n",
        "            predictions = model_fn(features)        \n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_train_loss += loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()        \n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        average_train_loss = total_train_loss / len(train_dataloader)\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"average_train_loss\"].append(average_train_loss.item())\n",
        "\n",
        "        # validation after every training epoch\n",
        "        model_fn = model_fn.eval()\n",
        "        with torch.inference_mode():\n",
        "            average_val_loss = compute_average_loss(\n",
        "                val_dataloader,\n",
        "                loss_fn,\n",
        "                model_fn\n",
        "            )\n",
        "        history[\"average_val_loss\"].append(average_val_loss.item())\n",
        "\n",
        "\n",
        "        if epoch % log_epochs == 0:\n",
        "            message = f\"Epoch {epoch}, Average train Loss {average_train_loss.item():.4f}, Average val Loss {average_val_loss.item():.4f}\"\n",
        "            print(message)\n",
        "\n",
        "    history_df = (pd.DataFrame.from_dict(history)\n",
        "                              .set_index(\"epoch\"))\n",
        "    return history_df\n"
      ],
      "metadata": {
        "id": "iBhCJguAJ9Cq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the MNIST data"
      ],
      "metadata": {
        "id": "WNqB4QAmOLaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat ./sample_data/mnist_train_small.csv | head -n 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ5JMTINJJmQ",
        "outputId": "51b74305-d7a7-403a-9d87-105232847c03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,24,67,67,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,131,252,252,66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,159,250,232,30,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,222,252,108,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,147,252,183,5,0,0,0,0,0,0,0,20,89,89,73,0,0,0,0,0,0,0,0,0,0,0,0,48,247,252,159,0,0,0,0,0,0,0,79,236,252,252,249,198,16,0,0,0,0,0,0,0,0,0,41,193,252,199,22,0,0,0,0,0,12,135,248,252,252,252,252,252,100,0,0,0,0,0,0,0,0,0,100,252,252,88,0,0,0,0,0,11,171,252,252,235,175,178,252,252,224,0,0,0,0,0,0,0,0,15,209,252,233,12,0,0,0,0,49,177,252,252,89,26,0,2,166,252,252,0,0,0,0,0,0,0,0,96,253,253,59,0,0,0,0,11,177,255,253,92,0,0,0,0,155,253,128,0,0,0,0,0,0,0,0,143,252,252,10,0,0,0,12,171,252,216,110,13,0,0,0,3,180,232,24,0,0,0,0,0,0,0,0,143,252,170,2,0,0,0,135,252,209,19,0,0,0,0,0,12,252,132,0,0,0,0,0,0,0,0,0,249,252,96,0,0,0,21,248,246,34,0,0,0,0,5,61,234,152,3,0,0,0,0,0,0,0,0,0,253,252,44,0,0,0,145,252,104,0,0,0,46,114,184,252,149,34,0,0,0,0,0,0,0,0,0,0,253,252,82,0,0,31,239,252,66,39,89,165,243,252,233,126,5,0,0,0,0,0,0,0,0,0,0,0,249,252,244,126,98,143,252,252,237,240,253,252,243,174,17,0,0,0,0,0,0,0,0,0,0,0,0,0,119,239,252,252,252,252,252,252,252,252,228,179,17,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,66,66,66,66,66,66,66,66,29,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,28,59,50,0,23,0,0,32,134,180,254,206,8,0,0,0,0,0,0,0,0,0,0,0,0,4,96,216,233,254,248,215,231,215,215,236,254,250,181,27,0,0,0,0,0,0,0,0,0,0,0,0,0,108,254,254,247,175,175,175,176,175,175,205,175,60,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,254,245,85,0,0,0,0,0,0,8,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,152,254,158,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,240,255,38,0,41,50,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,87,254,254,178,215,242,248,215,96,19,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,176,254,254,254,217,175,187,254,254,248,85,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,161,247,214,57,11,0,3,19,177,248,248,129,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,18,49,0,0,0,0,0,0,0,57,224,254,171,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,213,255,122,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,92,254,196,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,40,254,196,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,145,254,196,0,0,0,0,0,0,0,0,0,0,0,0,0,0,31,188,45,0,0,0,0,0,0,0,99,249,254,121,0,0,0,0,0,0,0,0,0,0,0,0,0,0,139,245,45,0,0,0,0,0,0,140,254,254,133,0,0,0,0,0,0,0,0,0,0,0,0,0,0,17,242,169,0,0,0,0,4,58,216,248,254,167,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,14,230,196,79,49,79,79,181,254,254,247,108,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,44,213,254,247,254,254,254,254,192,32,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,133,156,193,155,140,58,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "7,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,32,97,179,254,223,72,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,13,65,185,235,253,254,253,253,199,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,37,55,0,61,224,253,253,253,192,78,226,253,213,0,0,0,0,0,0,0,0,0,0,0,0,0,0,100,228,247,159,248,254,234,183,64,5,0,177,253,161,0,0,0,0,0,0,0,0,0,0,0,0,0,76,254,253,253,253,253,193,46,0,0,0,0,214,253,117,0,0,0,0,0,0,0,0,0,0,0,0,0,121,255,254,254,146,60,0,0,0,0,0,14,224,254,57,0,0,0,0,0,0,0,0,0,0,0,0,79,244,254,243,106,3,0,0,0,0,0,0,186,253,216,10,0,0,0,0,0,0,0,0,0,0,0,0,166,253,254,135,0,0,0,0,0,0,0,0,254,253,107,0,0,0,0,0,0,0,0,0,0,0,0,126,251,253,146,3,0,0,0,0,0,0,0,106,254,242,36,0,0,0,0,0,0,0,0,0,0,0,8,205,253,215,23,0,0,0,0,0,0,0,31,239,254,121,0,0,0,0,0,0,0,0,0,0,0,0,178,254,244,83,0,0,0,0,0,0,0,19,201,254,196,15,0,0,0,0,0,0,0,0,0,0,0,28,232,253,124,0,0,0,0,0,0,0,2,129,253,253,15,0,0,0,0,0,0,0,0,0,0,0,0,18,220,174,13,0,0,0,0,0,0,0,88,253,253,154,0,0,0,0,0,0,0,0,0,0,0,0,0,0,15,0,0,0,0,0,0,0,0,10,175,253,231,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,134,253,253,138,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,83,255,254,152,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,222,254,191,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,137,253,254,135,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,234,253,254,173,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,159,253,193,46,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,25,114,181,219,255,196,126,122,22,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,74,212,218,254,254,225,217,216,245,133,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,46,104,187,253,213,129,56,15,0,0,86,237,88,0,0,0,0,0,0,0,0,0,0,0,0,0,0,82,235,254,254,143,0,0,0,0,0,0,10,254,113,0,0,0,0,0,0,0,0,0,0,0,0,0,0,226,254,254,70,4,0,0,0,0,0,0,1,50,100,22,79,111,0,0,0,0,0,0,0,0,0,0,0,226,254,254,71,4,45,93,90,90,19,5,23,207,228,228,254,243,73,0,0,0,0,0,0,0,0,0,0,195,254,254,254,193,232,254,254,254,254,198,254,254,254,254,254,254,131,0,0,0,0,0,0,0,0,0,0,19,176,235,254,254,254,254,254,254,254,254,254,254,254,254,254,254,111,0,0,0,0,0,0,0,0,0,0,0,0,25,125,131,162,225,215,131,201,131,169,209,254,254,254,216,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,254,254,222,30,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,223,254,254,145,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,73,254,254,152,9,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,200,254,252,66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,101,254,254,143,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,80,254,254,95,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,157,254,213,10,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,23,235,254,85,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,88,254,251,66,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,160,254,153,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,91,225,15,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n",
            "5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,20,206,210,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,47,179,248,239,122,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,78,192,237,237,206,237,249,254,239,123,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,48,229,254,254,235,254,254,174,114,26,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,155,254,132,35,23,35,35,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,9,232,232,23,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,131,254,140,0,12,63,51,27,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,254,126,61,172,254,254,254,166,82,14,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,254,230,254,250,208,160,245,251,254,237,135,20,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,254,254,241,92,0,0,0,52,133,248,254,225,46,0,0,0,0,0,0,0,0,0,0,0,0,0,0,149,255,241,61,0,0,0,0,0,0,22,153,254,242,89,0,0,0,0,0,0,0,0,0,0,0,0,0,26,127,49,0,0,0,0,0,0,0,0,20,153,254,227,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,19,228,252,57,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,211,254,61,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,211,254,61,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,49,246,241,32,0,0,0,0,0,0,0,0,0,0,0,0,0,33,159,105,0,0,0,0,0,0,0,42,217,254,179,0,0,0,0,0,0,0,0,0,0,0,0,0,26,218,254,71,0,0,0,0,0,18,122,248,254,215,13,0,0,0,0,0,0,0,0,0,0,0,0,0,39,241,254,208,97,91,43,128,185,251,254,252,184,12,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,93,242,254,254,254,254,254,254,254,202,71,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_SIZE = 784\n",
        "OUTPUT_SIZE = 10\n",
        "\n",
        "_train_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_train_small.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")\n",
        "train_data, val_data = model_selection.train_test_split(\n",
        "    _train_data,\n",
        "    test_size=0.1,\n",
        "    stratify=_train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "test_data = pd.read_csv(\n",
        "    \"./sample_data/mnist_test.csv\",\n",
        "    header=None,\n",
        "    names=[\"label\"] + [f\"p{i}\" for i in range(INPUT_SIZE)],\n",
        ")"
      ],
      "metadata": {
        "id": "pR69GKYgI64h"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create preprocessing pipelines"
      ],
      "metadata": {
        "id": "lEerhXJQOUoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.MinMaxScaler(),\n",
        "    preprocessing.FunctionTransformer(lambda arr: arr.astype(np.float32)),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n",
        "\n",
        "target_preprocessor = pipeline.make_pipeline(\n",
        "    preprocessing.FunctionTransformer(lambda df: df.to_numpy()),\n",
        "    preprocessing.FunctionTransformer(lambda arr: torch.from_numpy(arr))\n",
        ")\n"
      ],
      "metadata": {
        "id": "IOvwdgWvOJsk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Datasets and DataLoaders"
      ],
      "metadata": {
        "id": "iYDuB5zYOZL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# create the training dataset and dataloader\n",
        "train_features_tensor = features_preprocessor.fit_transform(\n",
        "    train_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "train_target_tensor = target_preprocessor.fit_transform(\n",
        "    train_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "train_dataset = utils.data.TensorDataset(\n",
        "    train_features_tensor,\n",
        "    train_target_tensor\n",
        ")\n",
        "\n",
        "train_dataloader = utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the validation dataset and dataloader\n",
        "val_features_tensor = features_preprocessor.transform(\n",
        "    val_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "val_target_tensor = target_preprocessor.transform(\n",
        "    val_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "val_dataset = utils.data.TensorDataset(\n",
        "    val_features_tensor,\n",
        "    val_target_tensor\n",
        ")\n",
        "\n",
        "val_dataloader = utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n",
        "\n",
        "# create the test dataset and dataloader\n",
        "test_features_tensor = features_preprocessor.transform(\n",
        "    test_data.drop(\"label\", axis=1)\n",
        ")\n",
        "\n",
        "test_target_tensor = target_preprocessor.transform(\n",
        "    test_data.loc[:, \"label\"]\n",
        ")\n",
        "\n",
        "test_dataset = utils.data.TensorDataset(\n",
        "    test_features_tensor,\n",
        "    test_target_tensor\n",
        ")\n",
        "\n",
        "test_dataloader = utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        ")\n"
      ],
      "metadata": {
        "id": "3gIHandmN6MV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanishing gradients example"
      ],
      "metadata": {
        "id": "pPZ_cRdOOltl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE = 100\n",
        "LEARNING_RATE = 1e-2\n",
        "MAX_EPOCHS = 10\n",
        "\n",
        "model_fn = nn.Sequential(\n",
        "    nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(HIDDEN_SIZE, HIDDEN_SIZE),\n",
        "    nn.Sigmoid(),\n",
        "    nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE),\n",
        "    nn.LogSoftmax(dim=1),\n",
        ")\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXGDagLbTVhe",
        "outputId": "6fc789cb-0eb0-476d-cd97-10383ad8bfa1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.3056, Average val Loss 2.2995\n",
            "Epoch 1, Average train Loss 2.2996, Average val Loss 2.2980\n",
            "Epoch 2, Average train Loss 2.2981, Average val Loss 2.2964\n",
            "Epoch 3, Average train Loss 2.2965, Average val Loss 2.2948\n",
            "Epoch 4, Average train Loss 2.2947, Average val Loss 2.2929\n",
            "Epoch 5, Average train Loss 2.2928, Average val Loss 2.2909\n",
            "Epoch 6, Average train Loss 2.2906, Average val Loss 2.2886\n",
            "Epoch 7, Average train Loss 2.2881, Average val Loss 2.2859\n",
            "Epoch 8, Average train Loss 2.2852, Average val Loss 2.2828\n",
            "Epoch 9, Average train Loss 2.2818, Average val Loss 2.2791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the both train and val losses fail to decrease: the Sigmoid activation function has become saturated and as such the gradient is basically zero: parameters are not updating, model is not learning, so training is not making any progress."
      ],
      "metadata": {
        "id": "_-NYpBwWfXl-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8RRo5wlPJ9KL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Better Activation Functions\n",
        "\n",
        "In this section we will explore how different activation functions can address the problem of vanishing gradients. \n",
        "\n",
        "The code in the cell below defines another utility function to help us generate MLP classifiers with different activation functions."
      ],
      "metadata": {
        "id": "NiFSOQE-r-7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mlp_classifier(\n",
        "    input_size,\n",
        "    hidden_sizes=None,\n",
        "    output_size=2,\n",
        "    activation_fn=None):\n",
        "    modules = []\n",
        "    hidden_sizes = [] if hidden_sizes is None else hidden_sizes\n",
        "    for hidden_size in hidden_sizes:\n",
        "        hidden_layer = nn.Linear(input_size, hidden_size)\n",
        "        modules.append(hidden_layer)\n",
        "        if activation_fn is not None:\n",
        "            modules.append(activation_fn)\n",
        "        input_size=hidden_size\n",
        "    output_layer = nn.Linear(input_size, output_size)\n",
        "    modules.append(output_layer)\n",
        "    modules.append(nn.LogSoftmax(dim=1))\n",
        "    model_fn = nn.Sequential(*modules)\n",
        "    return model_fn, nn.NLLLoss()"
      ],
      "metadata": {
        "id": "V9vsPepWTWrc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU\n",
        "\n",
        "* ReLU does not suffer from vanishing gradients for positive values and is very fast to compute.\n",
        "\n",
        "* ReLU can suffer from the problem of \"dying\" ReLUs if too many neurons output negative values as the ReLU will then output zero."
      ],
      "metadata": {
        "id": "zPNPajkef7fQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.ReLU?"
      ],
      "metadata": {
        "id": "bGM2rQpUf9XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.ReLU()\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cduU9-omuNxi",
        "outputId": "b07389be-c9f7-4a51-cf52-5e6808c15cd5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.2412, Average val Loss 2.1458\n",
            "Epoch 1, Average train Loss 1.8716, Average val Loss 1.4871\n",
            "Epoch 2, Average train Loss 1.1156, Average val Loss 0.8570\n",
            "Epoch 3, Average train Loss 0.7152, Average val Loss 0.6424\n",
            "Epoch 4, Average train Loss 0.5658, Average val Loss 0.5510\n",
            "Epoch 5, Average train Loss 0.4905, Average val Loss 0.4991\n",
            "Epoch 6, Average train Loss 0.4432, Average val Loss 0.4648\n",
            "Epoch 7, Average train Loss 0.4102, Average val Loss 0.4406\n",
            "Epoch 8, Average train Loss 0.3860, Average val Loss 0.4223\n",
            "Epoch 9, Average train Loss 0.3674, Average val Loss 0.4082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leaky ReLU\n",
        "\n",
        "* Hyperparameter controls how much the activation function \"leaks\".\n",
        "* Having non-zero slope for negative ouputs solves the \"dying ReLUs\" problem."
      ],
      "metadata": {
        "id": "dZmowE54vJXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.LeakyReLU?"
      ],
      "metadata": {
        "id": "Rb7bQH6XvTEh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.LeakyReLU(negative_slope=0.01)\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QycJW9wNuNz4",
        "outputId": "bcde1b83-48fb-4d2f-b92c-d441ea2bdd0f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.2452, Average val Loss 2.1518\n",
            "Epoch 1, Average train Loss 1.8940, Average val Loss 1.5137\n",
            "Epoch 2, Average train Loss 1.1479, Average val Loss 0.8773\n",
            "Epoch 3, Average train Loss 0.7304, Average val Loss 0.6412\n",
            "Epoch 4, Average train Loss 0.5649, Average val Loss 0.5421\n",
            "Epoch 5, Average train Loss 0.4844, Average val Loss 0.4891\n",
            "Epoch 6, Average train Loss 0.4360, Average val Loss 0.4549\n",
            "Epoch 7, Average train Loss 0.4028, Average val Loss 0.4302\n",
            "Epoch 8, Average train Loss 0.3782, Average val Loss 0.4113\n",
            "Epoch 9, Average train Loss 0.3591, Average val Loss 0.3964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "There are a couple of variants of Leaky ReLU: Randomized Leaky ReLU and Parametric Leaky ReLU. Adapt the code above to train models using these variants. Plot the training and validation losses and discuss."
      ],
      "metadata": {
        "id": "krapsKaQjF9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.RReLU?"
      ],
      "metadata": {
        "id": "rssGot-YiuwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.PReLU?"
      ],
      "metadata": {
        "id": "_gfltCjaix1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1FYRcr9jecp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELU and SELU\n",
        "\n",
        "**Exponential Linear Unit (ELU)** is *negative* when neuron outputs a negative number. Has a hyperparameter that determines the value that the function approahes when neuron outputs are large and negative.\n",
        "\n",
        "**Scaled ELU (SELU)** often used when training MLPs as this activation function allows the network to self-normalize!"
      ],
      "metadata": {
        "id": "4GSdvcypEMTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.ELU?"
      ],
      "metadata": {
        "id": "Jck-axz4ESAK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.ELU(alpha=1.0)\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moDVHWjNVB2i",
        "outputId": "62dfa825-f37f-4c58-f861-5b4e1a49d7d3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.0847, Average val Loss 1.7655\n",
            "Epoch 1, Average train Loss 1.3292, Average val Loss 0.9811\n",
            "Epoch 2, Average train Loss 0.7929, Average val Loss 0.6858\n",
            "Epoch 3, Average train Loss 0.5930, Average val Loss 0.5630\n",
            "Epoch 4, Average train Loss 0.4961, Average val Loss 0.4968\n",
            "Epoch 5, Average train Loss 0.4395, Average val Loss 0.4563\n",
            "Epoch 6, Average train Loss 0.4032, Average val Loss 0.4297\n",
            "Epoch 7, Average train Loss 0.3785, Average val Loss 0.4110\n",
            "Epoch 8, Average train Loss 0.3606, Average val Loss 0.3971\n",
            "Epoch 9, Average train Loss 0.3468, Average val Loss 0.3861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.SELU?"
      ],
      "metadata": {
        "id": "heIsL_rFESEO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.SELU()\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEY4q3HzESNq",
        "outputId": "f899260a-88de-4464-aa8c-7017f16514d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 1.6407, Average val Loss 1.0191\n",
            "Epoch 1, Average train Loss 0.7501, Average val Loss 0.6132\n",
            "Epoch 2, Average train Loss 0.5182, Average val Loss 0.4937\n",
            "Epoch 3, Average train Loss 0.4310, Average val Loss 0.4380\n",
            "Epoch 4, Average train Loss 0.3854, Average val Loss 0.4063\n",
            "Epoch 5, Average train Loss 0.3572, Average val Loss 0.3854\n",
            "Epoch 6, Average train Loss 0.3377, Average val Loss 0.3703\n",
            "Epoch 7, Average train Loss 0.3230, Average val Loss 0.3588\n",
            "Epoch 8, Average train Loss 0.3113, Average val Loss 0.3495\n",
            "Epoch 9, Average train Loss 0.3016, Average val Loss 0.3418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GELU, Swish, Mish\n",
        "\n",
        "**Gaussian Error Linear Unit (GELU)**, **Sigmoid Linear Unit (SiLU or Swish)**, and **Mish** are all smooth, non-monotonic, non-convex, ReLU variants.\n",
        "\n",
        "The idea with these activation functions is that while the extra complexity of the functions (relative to ReLU) takes more compute time during training (and inference), the training process will converge to a good solution in fewer iterations."
      ],
      "metadata": {
        "id": "uP2NTVsoEqht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.GELU?"
      ],
      "metadata": {
        "id": "8KLmbMg_EyWO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.GELU(approximate=\"none\")\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qK1g0cwTEyZQ",
        "outputId": "384419a6-b368-4541-a307-5a7f22211399"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.2895, Average val Loss 2.2719\n",
            "Epoch 1, Average train Loss 2.2406, Average val Loss 2.1902\n",
            "Epoch 2, Average train Loss 2.0386, Average val Loss 1.8033\n",
            "Epoch 3, Average train Loss 1.3748, Average val Loss 1.0004\n",
            "Epoch 4, Average train Loss 0.8001, Average val Loss 0.6912\n",
            "Epoch 5, Average train Loss 0.5940, Average val Loss 0.5693\n",
            "Epoch 6, Average train Loss 0.4955, Average val Loss 0.5059\n",
            "Epoch 7, Average train Loss 0.4391, Average val Loss 0.4677\n",
            "Epoch 8, Average train Loss 0.4030, Average val Loss 0.4420\n",
            "Epoch 9, Average train Loss 0.3778, Average val Loss 0.4233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.SiLU?"
      ],
      "metadata": {
        "id": "0hUMj0XNFMHr"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.SiLU()\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbPu2kA8VrJn",
        "outputId": "b36742ca-6597-4640-8ea1-d42866967d10"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.2834, Average val Loss 2.2626\n",
            "Epoch 1, Average train Loss 2.2278, Average val Loss 2.1775\n",
            "Epoch 2, Average train Loss 2.0554, Average val Loss 1.8715\n",
            "Epoch 3, Average train Loss 1.5951, Average val Loss 1.2825\n",
            "Epoch 4, Average train Loss 1.0040, Average val Loss 0.8080\n",
            "Epoch 5, Average train Loss 0.6879, Average val Loss 0.6329\n",
            "Epoch 6, Average train Loss 0.5572, Average val Loss 0.5496\n",
            "Epoch 7, Average train Loss 0.4853, Average val Loss 0.5012\n",
            "Epoch 8, Average train Loss 0.4396, Average val Loss 0.4700\n",
            "Epoch 9, Average train Loss 0.4083, Average val Loss 0.4485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Mish?"
      ],
      "metadata": {
        "id": "loITivUtEybw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.Mish()\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aiolygqbEyjN",
        "outputId": "593255c8-a887-4174-a326-038c36d1e7ce"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 2.2665, Average val Loss 2.2188\n",
            "Epoch 1, Average train Loss 2.1056, Average val Loss 1.9295\n",
            "Epoch 2, Average train Loss 1.5795, Average val Loss 1.1802\n",
            "Epoch 3, Average train Loss 0.8998, Average val Loss 0.7299\n",
            "Epoch 4, Average train Loss 0.6209, Average val Loss 0.5850\n",
            "Epoch 5, Average train Loss 0.5125, Average val Loss 0.5176\n",
            "Epoch 6, Average train Loss 0.4538, Average val Loss 0.4770\n",
            "Epoch 7, Average train Loss 0.4160, Average val Loss 0.4498\n",
            "Epoch 8, Average train Loss 0.3896, Average val Loss 0.4302\n",
            "Epoch 9, Average train Loss 0.3700, Average val Loss 0.4154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Train a MLP with three hidden layers, each layer with 100 neurons per layer, for 10 epochs on the MNIST dataset using Stochastic Gradient Descent with a learning rate of 1e-2 for three different activations functions. Plot the training and validation loss (or accuracy) curves. Compare and contrast the differences across activation functions."
      ],
      "metadata": {
        "id": "uaCQM6TCFemd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PA0fqxdSEylU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Better Parameter Initialization Strategies\n",
        "\n",
        "With deeper models that have more parameters, choosing the right parameter initialization strategy can be critical."
      ],
      "metadata": {
        "id": "7ELQU7KPsByE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_linear_layer(\n",
        "    in_features,\n",
        "    out_features,\n",
        "    init_strategy_=nn.init.kaiming_uniform_):\n",
        "    linear_layer = nn.Linear(in_features, out_features)\n",
        "    init_strategy_(linear_layer.weight)\n",
        "    return linear_layer\n"
      ],
      "metadata": {
        "id": "xvB6WJuvTyiL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mlp_classifier(\n",
        "    input_size,\n",
        "    hidden_sizes=None,\n",
        "    output_size=2,\n",
        "    activation_fn=None,\n",
        "    init_strategy_=nn.init.kaiming_uniform_):\n",
        "    modules = []\n",
        "    hidden_sizes = [] if hidden_sizes is None else hidden_sizes\n",
        "    for hidden_size in hidden_sizes:\n",
        "        hidden_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            init_strategy_,\n",
        "        )\n",
        "        modules.append(hidden_layer)\n",
        "        if activation_fn is not None:\n",
        "            modules.append(activation_fn)\n",
        "        input_size=hidden_size\n",
        "    output_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            output_size,\n",
        "            init_strategy_,\n",
        "    )\n",
        "    modules.append(output_layer)\n",
        "    modules.append(nn.LogSoftmax(dim=1))\n",
        "    model_fn = nn.Sequential(*modules)\n",
        "    return model_fn, nn.NLLLoss()"
      ],
      "metadata": {
        "id": "qaHp1XiAKwoE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kaiming"
      ],
      "metadata": {
        "id": "bW5kt-QSYxhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.init.kaiming_normal_?"
      ],
      "metadata": {
        "id": "cu_myI9gM_cG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.init.kaiming_uniform_?"
      ],
      "metadata": {
        "id": "99zoyKpjNCQw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.ReLU(),\n",
        "    init_strategy_=nn.init.kaiming_uniform_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv7BJGZNYmsw",
        "outputId": "1b5ae7a1-2cd5-4beb-81a7-756d580699d7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 1.3993, Average val Loss 0.7746\n",
            "Epoch 1, Average train Loss 0.5853, Average val Loss 0.5064\n",
            "Epoch 2, Average train Loss 0.4283, Average val Loss 0.4255\n",
            "Epoch 3, Average train Loss 0.3656, Average val Loss 0.3845\n",
            "Epoch 4, Average train Loss 0.3299, Average val Loss 0.3581\n",
            "Epoch 5, Average train Loss 0.3055, Average val Loss 0.3385\n",
            "Epoch 6, Average train Loss 0.2869, Average val Loss 0.3228\n",
            "Epoch 7, Average train Loss 0.2717, Average val Loss 0.3095\n",
            "Epoch 8, Average train Loss 0.2589, Average val Loss 0.2975\n",
            "Epoch 9, Average train Loss 0.2478, Average val Loss 0.2871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Xavier"
      ],
      "metadata": {
        "id": "8RXrE_ZeLsav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.init.xavier_normal_?"
      ],
      "metadata": {
        "id": "o2C_olL-KWKH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.init.xavier_uniform_?"
      ],
      "metadata": {
        "id": "w5BP4ibxLxTC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.ReLU(),\n",
        "    init_strategy_=nn.init.xavier_normal_\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqWkBhqeZs13",
        "outputId": "99d36310-da1e-4ba7-f40e-c8926a51caee"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 1.7259, Average val Loss 1.0496\n",
            "Epoch 1, Average train Loss 0.7400, Average val Loss 0.5965\n",
            "Epoch 2, Average train Loss 0.5029, Average val Loss 0.4838\n",
            "Epoch 3, Average train Loss 0.4217, Average val Loss 0.4326\n",
            "Epoch 4, Average train Loss 0.3786, Average val Loss 0.4020\n",
            "Epoch 5, Average train Loss 0.3504, Average val Loss 0.3805\n",
            "Epoch 6, Average train Loss 0.3298, Average val Loss 0.3641\n",
            "Epoch 7, Average train Loss 0.3134, Average val Loss 0.3504\n",
            "Epoch 8, Average train Loss 0.2999, Average val Loss 0.3387\n",
            "Epoch 9, Average train Loss 0.2882, Average val Loss 0.3285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Choose the appropriate activation function and initialization strategy to implement a self-normalizing MLP. Your MLP should have three hidden layers, each layer with 100 neurons per layer. Train your MLP for 10 epochs on the MNIST dataset using Stochastic Gradient Descent with a learning rate of 1e-2 for the appropriately chose. Plot the training and validation loss (or accuracy) curves."
      ],
      "metadata": {
        "id": "D9HA8QEnPEy1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqoWw1ZEPkQQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Normalization"
      ],
      "metadata": {
        "id": "EO4M3vQOsPX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.BatchNorm1d?"
      ],
      "metadata": {
        "id": "kUx49GVrHc1B"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mlp_classifier(\n",
        "    input_size,\n",
        "    hidden_sizes=None,\n",
        "    output_size=2,\n",
        "    activation_fn=None,\n",
        "    init_strategy_=nn.init.kaiming_uniform_,\n",
        "    batch_normalization=False):\n",
        "    modules = []\n",
        "    hidden_sizes = [] if hidden_sizes is None else hidden_sizes\n",
        "    for hidden_size in hidden_sizes:\n",
        "        hidden_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            hidden_size,\n",
        "            init_strategy_,\n",
        "        )\n",
        "        modules.append(hidden_layer)\n",
        "        if batch_normalization:\n",
        "            modules.append(nn.BatchNorm1d(hidden_size))\n",
        "        if activation_fn is not None:\n",
        "            modules.append(activation_fn)\n",
        "        input_size=hidden_size\n",
        "    output_layer = initialize_linear_layer(\n",
        "            input_size,\n",
        "            output_size,\n",
        "            init_strategy_,\n",
        "    )\n",
        "    modules.append(output_layer)\n",
        "    modules.append(nn.LogSoftmax(dim=1))\n",
        "    model_fn = nn.Sequential(*modules)\n",
        "    return model_fn, nn.NLLLoss()"
      ],
      "metadata": {
        "id": "NK4wtfDXWren"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.ReLU(),\n",
        "    init_strategy_=nn.init.kaiming_uniform_,\n",
        "    batch_normalization=True\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Mx8gJqPwT5",
        "outputId": "731303b2-4bc5-41fc-8f73-e02b02dfa3f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 1.0466, Average val Loss 0.6011\n",
            "Epoch 1, Average train Loss 0.4988, Average val Loss 0.4253\n",
            "Epoch 2, Average train Loss 0.3780, Average val Loss 0.3521\n",
            "Epoch 3, Average train Loss 0.3158, Average val Loss 0.3109\n",
            "Epoch 4, Average train Loss 0.2756, Average val Loss 0.2832\n",
            "Epoch 5, Average train Loss 0.2464, Average val Loss 0.2631\n",
            "Epoch 6, Average train Loss 0.2236, Average val Loss 0.2479\n",
            "Epoch 7, Average train Loss 0.2049, Average val Loss 0.2358\n",
            "Epoch 8, Average train Loss 0.1892, Average val Loss 0.2261\n",
            "Epoch 9, Average train Loss 0.1756, Average val Loss 0.2176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Add BatchNormalization to your MLP. Your MLP should have three hidden layers, each layer with 100 neurons per layer. Train your MLP for 10 epochs on the MNIST dataset using Stochastic Gradient Descent with a learning rate of 1e-2 for the appropriately chose. Plot the training and validation loss (or accuracy) curves."
      ],
      "metadata": {
        "id": "57xgBrpyPxHL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BPgZFbajP7up"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Clipping"
      ],
      "metadata": {
        "id": "S01XtdVPXvve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn.utils.clip_grad_value_?"
      ],
      "metadata": {
        "id": "RF768-6sRqm9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn.utils.clip_grad_norm_?"
      ],
      "metadata": {
        "id": "Ap978b9WRqpX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_gradients_(\n",
        "    clip_grad_strategy,\n",
        "    model_fn,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "    if clip_grad_strategy == \"value\" and clip_value is not None:\n",
        "        nn.utils.clip_grad_value_(\n",
        "            model_fn.parameters(),\n",
        "            clip_value\n",
        "        )\n",
        "    elif clip_grad_strategy == \"norm\" and max_norm is not None:\n",
        "        nn.utils.clip_grad_norm_(\n",
        "            model_fn.parameters(),\n",
        "            max_norm,\n",
        "            norm_type,\n",
        "            error_if_nonfinite\n",
        "        )\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "def compute_average_loss(dataloader, loss_fn, model_fn):\n",
        "    total_loss = torch.zeros(1, 1)\n",
        "    for features, targets in dataloader:\n",
        "        predictions = model_fn(features)        \n",
        "        total_loss += loss_fn(predictions, targets)\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    return average_loss\n",
        "\n",
        "\n",
        "def fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    clip_grad_strategy=None,\n",
        "    clip_value=None,\n",
        "    error_if_nonfinite=False,\n",
        "    log_epochs=1,\n",
        "    max_epochs=1,\n",
        "    max_norm=None,\n",
        "    norm_type=2.0):\n",
        "  \n",
        "    history = {\n",
        "        \"epoch\": [],\n",
        "        \"average_train_loss\": [],\n",
        "        \"average_val_loss\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        total_train_loss = torch.zeros(1, 1)\n",
        "        model_fn = model_fn.train()\n",
        "        for features, targets in train_dataloader:\n",
        "            \n",
        "            # forward pass\n",
        "            predictions = model_fn(features)        \n",
        "            loss = loss_fn(predictions, targets)\n",
        "            total_train_loss += loss\n",
        "\n",
        "            # backward pass\n",
        "            loss.backward()\n",
        "            clip_gradients_(\n",
        "                clip_grad_strategy,\n",
        "                model_fn,\n",
        "                clip_value,\n",
        "                error_if_nonfinite,\n",
        "                max_norm,\n",
        "                norm_type\n",
        "            )\n",
        "            optimizer.step()        \n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        average_train_loss = total_train_loss / len(train_dataloader)\n",
        "        history[\"epoch\"].append(epoch)\n",
        "        history[\"average_train_loss\"].append(average_train_loss.item())\n",
        "\n",
        "        # validation after every training epoch\n",
        "        model_fn = model_fn.eval()\n",
        "        with torch.inference_mode():\n",
        "            average_val_loss = compute_average_loss(\n",
        "                val_dataloader,\n",
        "                loss_fn,\n",
        "                model_fn\n",
        "            )\n",
        "        history[\"average_val_loss\"].append(average_val_loss.item())\n",
        "\n",
        "\n",
        "        if epoch % log_epochs == 0:\n",
        "            message = f\"Epoch {epoch}, Average train Loss {average_train_loss.item():.4f}, Average val Loss {average_val_loss.item():.4f}\"\n",
        "            print(message)\n",
        "\n",
        "    history_df = (pd.DataFrame.from_dict(history)\n",
        "                              .set_index(\"epoch\"))\n",
        "    return history_df\n"
      ],
      "metadata": {
        "id": "2PNAlSKkYGFe"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fn, loss_fn = make_mlp_classifier(\n",
        "    input_size=INPUT_SIZE, \n",
        "    hidden_sizes=[HIDDEN_SIZE, HIDDEN_SIZE], \n",
        "    output_size=OUTPUT_SIZE, \n",
        "    activation_fn=nn.ReLU(),\n",
        "    init_strategy_=nn.init.kaiming_uniform_,\n",
        "    batch_normalization=False,\n",
        ")\n",
        "\n",
        "optimizer = optim.SGD(model_fn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "history = fit(\n",
        "    loss_fn,\n",
        "    model_fn,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    clip_grad_strategy=\"norm\",\n",
        "    max_norm=1.0,\n",
        "    error_if_nonfinite=True,\n",
        "    max_epochs=MAX_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4Ch42EaRLNf",
        "outputId": "494f6aeb-3ec5-4821-d6fe-84652a06edf7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average train Loss 1.6808, Average val Loss 1.0832\n",
            "Epoch 1, Average train Loss 0.7940, Average val Loss 0.6429\n",
            "Epoch 2, Average train Loss 0.5382, Average val Loss 0.5134\n",
            "Epoch 3, Average train Loss 0.4442, Average val Loss 0.4538\n",
            "Epoch 4, Average train Loss 0.3948, Average val Loss 0.4184\n",
            "Epoch 5, Average train Loss 0.3632, Average val Loss 0.3939\n",
            "Epoch 6, Average train Loss 0.3406, Average val Loss 0.3753\n",
            "Epoch 7, Average train Loss 0.3230, Average val Loss 0.3603\n",
            "Epoch 8, Average train Loss 0.3084, Average val Loss 0.3474\n",
            "Epoch 9, Average train Loss 0.2959, Average val Loss 0.3364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise:\n",
        "\n",
        "Create an MLP with three hidden layers, each layer with 100 neurons per layer. Train your MLP for 10 epochs on the MNIST dataset using Stochastic Gradient Descent with a learning rate of 1e-2 and clip gradients by value and then repeat the training process and clip gradients by norm. Plot the training and validation loss (or accuracy) curves. Discuss."
      ],
      "metadata": {
        "id": "K07EMYVMRLxR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UPyM1ojyRpSE"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}